{
  "configuration": "配置",
  "model": "模型",
  "token": {
    "label": "最大回复文本数",
    "description": "规定模型生成的最大回复文本数，这一值越大，模型生成的回复文本越长。"
  },
  "default": "默认",
  "temperature": {
    "label": "模型发散程度",
    "description": "规定模型的发散程度。一个小的值会让模型生成的内容更加稳定，而一个更大的值允许模型生成更发散的内容，但这可能会加大模型生成不正确内容的可能性。(接受范围：0-2)"
  },
  "presencePenalty": {
    "label": "存在惩罚",
    "description": "数值在 -2.0 到 2.0 之间。正值会根据新 token 是否已经出现在文本中来惩罚它们，增加模型谈论新话题的可能性。 (默认: 0)"
  },
  "topP": {
    "label": "Top-p",
    "description": "数值在 0 到 1 之间。采用核采样（nucleus sampling）的一种采样温度的替代方法，模型仅考虑前 Top-p 概率质量的 token。因此，0.1 表示仅考虑前 10% 概率质量的 token。我们通常建议修改此参数或采样温度，但不要同时修改两者。(默认: 1)"
  },
  "frequencyPenalty": {
    "label": "频率惩罚",
    "description": "数值在 -2.0 到 2.0 之间。正值会根据新 token 在文本中的现有频率来惩罚它们，降低模型直接重复相同语句的可能性。(默认: 0)"
  },
  "defaultChatConfig": "默认聊天配置",
  "defaultSystemMessage": "默认系统消息",
  "resetToDefault": "重置为默认值"
}
